#!/usr/bin/env python3
"""
FJSPç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
æµ‹è¯•ç³»ç»Ÿçš„å„é¡¹æ€§èƒ½æŒ‡æ ‡å’Œå¤šç›®æ ‡ä¼˜åŒ–åŠŸèƒ½
"""

import time
import json
import requests
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
import concurrent.futures
import psutil
import os

class FJSPPerformanceBenchmark:
    def __init__(self, api_base="http://localhost:5001/api"):
        self.api_base = api_base
        self.results = {}
        self.test_instances = self.generate_test_instances()
        
    def generate_test_instances(self):
        """ç”Ÿæˆä¸åŒè§„æ¨¡çš„æµ‹è¯•å®ä¾‹"""
        instances = [
            {"name": "small", "num_jobs": 5, "num_machines": 3, "max_ops": 3},
            {"name": "medium", "num_jobs": 10, "num_machines": 5, "max_ops": 4},
            {"name": "large", "num_jobs": 20, "num_machines": 10, "max_ops": 5},
            {"name": "xlarge", "num_jobs": 50, "num_machines": 20, "max_ops": 6}
        ]
        return instances
    
    def test_api_performance(self):
        """æµ‹è¯•APIå“åº”æ€§èƒ½"""
        print("ğŸ” æµ‹è¯•APIæ€§èƒ½...")
        
        api_tests = [
            ("health_check", "GET", "/health", None),
            ("create_instance", "POST", "/instances", {
                "type": "random", "num_jobs": 5, "num_machines": 3, 
                "max_operations_per_job": 3, "flexibility": 0.7
            })
        ]
        
        api_results = {}
        
        for test_name, method, endpoint, data in api_tests:
            times = []
            success_count = 0
            
            for _ in range(10):  # æ¯ä¸ªAPIæµ‹è¯•10æ¬¡
                start_time = time.time()
                try:
                    if method == "GET":
                        response = requests.get(f"{self.api_base}{endpoint}", timeout=5)
                    else:
                        response = requests.post(f"{self.api_base}{endpoint}", 
                                               json=data, timeout=10)
                    
                    if response.status_code in [200, 201]:
                        success_count += 1
                    
                    times.append(time.time() - start_time)
                    
                except Exception as e:
                    print(f"âŒ APIæµ‹è¯•å¤±è´¥: {e}")
                    times.append(float('inf'))
            
            api_results[test_name] = {
                "avg_response_time": np.mean(times),
                "min_response_time": np.min(times),
                "max_response_time": np.max(times),
                "success_rate": success_count / 10,
                "std_response_time": np.std(times)
            }
        
        self.results["api_performance"] = api_results
        return api_results
    
    def test_scalability(self):
        """æµ‹è¯•ç³»ç»Ÿå¯æ‰©å±•æ€§"""
        print("ğŸ“ˆ æµ‹è¯•ç³»ç»Ÿå¯æ‰©å±•æ€§...")
        
        scalability_results = {}
        
        for instance_config in self.test_instances:
            print(f"  æµ‹è¯•å®ä¾‹: {instance_config['name']}")
            
            # åˆ›å»ºå®ä¾‹
            start_time = time.time()
            try:
                response = requests.post(f"{self.api_base}/instances", 
                                       json={
                                           "type": "random",
                                           "num_jobs": instance_config["num_jobs"],
                                           "num_machines": instance_config["num_machines"],
                                           "max_operations_per_job": instance_config["max_ops"],
                                           "flexibility": 0.7
                                       }, timeout=30)
                
                creation_time = time.time() - start_time
                
                if response.status_code == 200:
                    instance_data = response.json()
                    
                    scalability_results[instance_config["name"]] = {
                        "creation_time": creation_time,
                        "num_jobs": instance_config["num_jobs"],
                        "num_machines": instance_config["num_machines"],
                        "num_operations": instance_data.get("num_operations", 0),
                        "memory_usage": self.get_memory_usage(),
                        "status": "success"
                    }
                else:
                    scalability_results[instance_config["name"]] = {
                        "status": "failed",
                        "error": f"HTTP {response.status_code}"
                    }
                    
            except Exception as e:
                scalability_results[instance_config["name"]] = {
                    "status": "failed",
                    "error": str(e)
                }
        
        self.results["scalability"] = scalability_results
        return scalability_results
    
    def test_visualization_performance(self):
        """æµ‹è¯•å¯è§†åŒ–æ€§èƒ½"""
        print("ğŸ¨ æµ‹è¯•å¯è§†åŒ–æ€§èƒ½...")
        
        # é¦–å…ˆåˆ›å»ºä¸€ä¸ªæµ‹è¯•å®ä¾‹
        response = requests.post(f"{self.api_base}/instances", 
                               json={
                                   "type": "random", "num_jobs": 10, "num_machines": 5,
                                   "max_operations_per_job": 4, "flexibility": 0.7
                               })
        
        if response.status_code != 200:
            return {"error": "æ— æ³•åˆ›å»ºæµ‹è¯•å®ä¾‹"}
        
        instance_id = response.json()["instance_id"]
        
        viz_tests = [
            ("disjunctive_graph_spring", f"/instances/{instance_id}/visualize/disjunctive_graph?layout=spring"),
            ("disjunctive_graph_hierarchical", f"/instances/{instance_id}/visualize/disjunctive_graph?layout=hierarchical"),
            ("disjunctive_graph_random", f"/instances/{instance_id}/visualize/disjunctive_graph?layout=random")
        ]
        
        viz_results = {}
        
        for test_name, endpoint in viz_tests:
            start_time = time.time()
            try:
                response = requests.get(f"{self.api_base}{endpoint}", timeout=30)
                generation_time = time.time() - start_time
                
                if response.status_code == 200:
                    content_size = len(response.content)
                    viz_results[test_name] = {
                        "generation_time": generation_time,
                        "content_size": content_size,
                        "status": "success"
                    }
                else:
                    viz_results[test_name] = {
                        "status": "failed",
                        "error": f"HTTP {response.status_code}"
                    }
                    
            except Exception as e:
                viz_results[test_name] = {
                    "status": "failed",
                    "error": str(e)
                }
        
        self.results["visualization"] = viz_results
        return viz_results
    
    def test_concurrent_performance(self):
        """æµ‹è¯•å¹¶å‘æ€§èƒ½"""
        print("âš¡ æµ‹è¯•å¹¶å‘æ€§èƒ½...")
        
        def create_instance():
            """å¹¶å‘åˆ›å»ºå®ä¾‹çš„å‡½æ•°"""
            try:
                response = requests.post(f"{self.api_base}/instances", 
                                       json={
                                           "type": "random", "num_jobs": 5, "num_machines": 3,
                                           "max_operations_per_job": 3, "flexibility": 0.7
                                       }, timeout=15)
                return {
                    "status": "success" if response.status_code == 200 else "failed",
                    "response_time": response.elapsed.total_seconds(),
                    "status_code": response.status_code
                }
            except Exception as e:
                return {"status": "error", "error": str(e)}
        
        concurrent_levels = [1, 5, 10, 20]
        concurrent_results = {}
        
        for level in concurrent_levels:
            print(f"  æµ‹è¯•å¹¶å‘çº§åˆ«: {level}")
            start_time = time.time()
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=level) as executor:
                futures = [executor.submit(create_instance) for _ in range(level)]
                results = [future.result() for future in concurrent.futures.as_completed(futures)]
            
            total_time = time.time() - start_time
            success_count = sum(1 for r in results if r["status"] == "success")
            
            concurrent_results[f"level_{level}"] = {
                "total_time": total_time,
                "success_rate": success_count / level,
                "throughput": level / total_time,
                "avg_response_time": np.mean([r.get("response_time", 0) for r in results if "response_time" in r])
            }
        
        self.results["concurrent"] = concurrent_results
        return concurrent_results
    
    def get_memory_usage(self):
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        process = psutil.Process(os.getpid())
        return {
            "rss": process.memory_info().rss / 1024 / 1024,  # MB
            "vms": process.memory_info().vms / 1024 / 1024,  # MB
            "percent": process.memory_percent()
        }
    
    def get_system_info(self):
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        return {
            "cpu_count": psutil.cpu_count(),
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_total": psutil.virtual_memory().total / 1024 / 1024 / 1024,  # GB
            "memory_available": psutil.virtual_memory().available / 1024 / 1024 / 1024,  # GB
            "memory_percent": psutil.virtual_memory().percent
        }
    
    def run_full_benchmark(self):
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•"""
        print("ğŸ­ å¼€å§‹FJSPç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 50)
        
        # è®°å½•ç³»ç»Ÿä¿¡æ¯
        self.results["system_info"] = self.get_system_info()
        self.results["timestamp"] = datetime.now().isoformat()
        
        # è¿è¡Œå„é¡¹æµ‹è¯•
        try:
            self.test_api_performance()
            self.test_scalability()
            self.test_visualization_performance()
            self.test_concurrent_performance()
            
            # ç”ŸæˆæŠ¥å‘Š
            self.generate_report()
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return False
        
        return True
    
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š...")
        
        # ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†ç»“æœ
        with open("performance_benchmark_results.json", "w", encoding="utf-8") as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report()
        
        # ç”Ÿæˆå›¾è¡¨
        self.generate_charts()
        
        print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ:")
        print("  - performance_benchmark_results.json (è¯¦ç»†æ•°æ®)")
        print("  - performance_report.md (MarkdownæŠ¥å‘Š)")
        print("  - performance_charts.png (æ€§èƒ½å›¾è¡¨)")
    
    def generate_markdown_report(self):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        report = f"""# FJSPç³»ç»Ÿæ€§èƒ½æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚è¿°
- **æµ‹è¯•æ—¶é—´**: {self.results['timestamp']}
- **ç³»ç»Ÿé…ç½®**: {self.results['system_info']['cpu_count']}æ ¸CPU, {self.results['system_info']['memory_total']:.1f}GBå†…å­˜

## APIæ€§èƒ½æµ‹è¯•
"""
        
        if "api_performance" in self.results:
            for test_name, metrics in self.results["api_performance"].items():
                report += f"""
### {test_name}
- å¹³å‡å“åº”æ—¶é—´: {metrics['avg_response_time']:.3f}s
- æˆåŠŸç‡: {metrics['success_rate']:.1%}
- å“åº”æ—¶é—´æ ‡å‡†å·®: {metrics['std_response_time']:.3f}s
"""
        
        report += "\n## å¯æ‰©å±•æ€§æµ‹è¯•\n"
        if "scalability" in self.results:
            report += "| å®ä¾‹è§„æ¨¡ | å·¥ä»¶æ•° | æœºå™¨æ•° | åˆ›å»ºæ—¶é—´(s) | å†…å­˜ä½¿ç”¨(MB) | çŠ¶æ€ |\n"
            report += "|---------|--------|--------|-------------|--------------|------|\n"
            
            for instance_name, metrics in self.results["scalability"].items():
                if metrics.get("status") == "success":
                    report += f"| {instance_name} | {metrics['num_jobs']} | {metrics['num_machines']} | {metrics['creation_time']:.3f} | {metrics['memory_usage']['rss']:.1f} | âœ… |\n"
                else:
                    report += f"| {instance_name} | - | - | - | - | âŒ |\n"
        
        with open("performance_report.md", "w", encoding="utf-8") as f:
            f.write(report)
    
    def generate_charts(self):
        """ç”Ÿæˆæ€§èƒ½å›¾è¡¨"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('FJSPç³»ç»Ÿæ€§èƒ½æµ‹è¯•ç»“æœ', fontsize=16)
        
        # APIå“åº”æ—¶é—´å›¾è¡¨
        if "api_performance" in self.results:
            api_data = self.results["api_performance"]
            test_names = list(api_data.keys())
            response_times = [api_data[name]["avg_response_time"] for name in test_names]
            
            axes[0, 0].bar(test_names, response_times)
            axes[0, 0].set_title('APIå¹³å‡å“åº”æ—¶é—´')
            axes[0, 0].set_ylabel('æ—¶é—´ (ç§’)')
            axes[0, 0].tick_params(axis='x', rotation=45)
        
        # å¯æ‰©å±•æ€§å›¾è¡¨
        if "scalability" in self.results:
            scalability_data = self.results["scalability"]
            instance_names = []
            creation_times = []
            job_counts = []
            
            for name, metrics in scalability_data.items():
                if metrics.get("status") == "success":
                    instance_names.append(name)
                    creation_times.append(metrics["creation_time"])
                    job_counts.append(metrics["num_jobs"])
            
            if creation_times:
                axes[0, 1].plot(job_counts, creation_times, 'o-')
                axes[0, 1].set_title('å®ä¾‹åˆ›å»ºæ—¶é—´ vs å·¥ä»¶æ•°')
                axes[0, 1].set_xlabel('å·¥ä»¶æ•°')
                axes[0, 1].set_ylabel('åˆ›å»ºæ—¶é—´ (ç§’)')
        
        # å¹¶å‘æ€§èƒ½å›¾è¡¨
        if "concurrent" in self.results:
            concurrent_data = self.results["concurrent"]
            levels = [int(k.split('_')[1]) for k in concurrent_data.keys()]
            throughputs = [concurrent_data[f"level_{level}"]["throughput"] for level in levels]
            
            axes[1, 0].plot(levels, throughputs, 'o-')
            axes[1, 0].set_title('å¹¶å‘ååé‡')
            axes[1, 0].set_xlabel('å¹¶å‘çº§åˆ«')
            axes[1, 0].set_ylabel('ååé‡ (è¯·æ±‚/ç§’)')
        
        # å¯è§†åŒ–æ€§èƒ½å›¾è¡¨
        if "visualization" in self.results:
            viz_data = self.results["visualization"]
            viz_names = []
            generation_times = []
            
            for name, metrics in viz_data.items():
                if metrics.get("status") == "success":
                    viz_names.append(name.replace("disjunctive_graph_", ""))
                    generation_times.append(metrics["generation_time"])
            
            if generation_times:
                axes[1, 1].bar(viz_names, generation_times)
                axes[1, 1].set_title('å¯è§†åŒ–ç”Ÿæˆæ—¶é—´')
                axes[1, 1].set_ylabel('æ—¶é—´ (ç§’)')
                axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig('performance_charts.png', dpi=300, bbox_inches='tight')
        plt.close()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨FJSPç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•")
    
    # æ£€æŸ¥APIè¿æ¥
    try:
        response = requests.get("http://localhost:5001/api/health", timeout=5)
        if response.status_code != 200:
            print("âŒ æ— æ³•è¿æ¥åˆ°FJSP APIæœåŠ¡ï¼Œè¯·ç¡®ä¿æœåŠ¡æ­£åœ¨è¿è¡Œ")
            print("ğŸ’¡ è¿è¡Œ ./start_system.sh å¯åŠ¨æœåŠ¡")
            return
    except Exception as e:
        print(f"âŒ APIè¿æ¥å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿FJSPç³»ç»Ÿæ­£åœ¨è¿è¡Œ")
        return
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark = FJSPPerformanceBenchmark()
    success = benchmark.run_full_benchmark()
    
    if success:
        print("\nğŸ‰ æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆï¼")
        print("ğŸ“Š æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶äº†è§£è¯¦ç»†ç»“æœ")
    else:
        print("\nâŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥")

if __name__ == "__main__":
    main()
