##############################################################################
###                         Training                                       ###
##############################################################################

# (R)   [String]  RL algorithm you want to use - This template is for PPO
algorithm: ppo
# (R)   [string]  Path to the file with generated data that to be used for training
instances_file: jssp/config_job3_task4_tools0.pkl
# (O)   [string]  The finished model is saved under this name. Alternatively set to <automatic>, then it will be
                  # replaced with the current DayMonthYearHourMinute
saved_model_name: example_ppo_agent
# (R)   [int]     Seed for all pseudo random generators (random, numpy, torch)
seed: 0
# (O)   [bool]    Bool, if the train-test-split of instances should remain the same (1111) and be independent of the
                  # random seed. This is useful for hyperparameter-sweeps with multiple random seeds to keep the same
                  # test instances for comparability. Irrelevant, if the random seed across runs remains the same.
overwrite_split_seed: False
# (R)   [string]  Set an individual description that you can identify this training run more easily later on.
                  # This will be used in "weights and biases" as well
config_description: tasks_3x4
# (O)   [string]  Set a directory where you want to save the agent model
experiment_save_path: models
# (O)   [int]:    Wandb mode choose: choose from [0: no wandb, 1: wandb_offline, 2: wandb_online]
wandb_mode: 0
# (O)   [string]  Set a wandb project where you want to upload all wandb logs
wandb_project: ppo_test

# --- PPO parameter ---
# (O)   [int]     Number of steps collected before PPO updates the policy again
rollout_steps: 2048
# (O)   [float]   Factor to discount future rewards
gamma: 0.99
# (O)   [int]     Number of epochs the network gets fed with the whole rollout data, when training is triggered
n_epochs: 5
# (O)   [int]     Batch size into which the rollout data gets split
batch_size: 256
# (O)   [float]   Range of the acceptable deviation between the policy before and after training
clip_range: 0.2
# (O)   [float]   Entropy loss coefficient for the total loss calculation
ent_coef: 0.0
# (O)   [float]   Learning rate for the network updates
learning_rate: 0.002
# (O) List[int] List with dimension for the hidden layers (length of list = number of hidden layers) used in the policy net
policy_layer:  [ 256, 256 ]
# (O) [str] String for the activation function of the policy net
            # Note, the activation function has to be from the torch.nn module (e.g. ReLU)
policy_activation: 'ReLU'
# (O) List[int] List with dimension for the hidden layers (length of list = number of hidden layers) used in the value net
value_layer:  [ 256, 256 ]
# (O) [str] String for the activation function of the value net
            # Note, the activation function has to be from the torch.nn module (e.g. ReLU)
value_activation: 'ReLU'
# (R)   [int]     Maximum number of instances shown to the agent. Limits the training process. Note that instances may be
                  # multiple times, if total_instances is larger than the number of generated instances
total_instances: 100_000
# (R)   [int]     Maximum number of steps that the agent can interact with the env. Limits the training process
total_timesteps: 3_000_000
# (R)   [float]   Range between 0 and 1. How much (percentually) of the generated data will be used for training.
train_test_split: 0.8
# (R)   [float]   Range between 0 and 1. How much (percentually) of the remaining data (1-train_test_split) will be
                  # used for training.
test_validation_split: 0.8
# (R)   [int]     Number of environment step calls between intermediate (validation) tests
intermediate_test_interval: 1_000

# --- env (Environment) parameter ---
# (R)   [str]     Environment you want to use. The vanilla case is env_tetris_scheduling.
environment: env_tetris_scheduling_indirect_action
# (O)   [int]     Maximum number of steps the agent can take before the env interrupts the episode.
                  # Should be greater than the minimum number of agent actions required to solve the problem.
                  # Can be larger that the minimum number of agent actions, if e.g. invalid actions or skip actions are
                  # implemented
num_steps_max: 90
# (O)   [int]     After this number of episodes, the env prints the last episode result in the console
log_interval: 2
# (O)   [bool]    All initial task instances are shuffled before being returned to the agent as observation
shuffle: False
# (O)   [str]     The reward strategy determines, how the reward is computed. Default is 'dense_makespan_reward'
reward_strategy: dense_makespan_reward
# (O)   [int]     The reward scale is a float by which the reward is multiplied to increase/decrease the reward signal
                  # strength
reward_scale: 1

# --- benchmarking
# (R)   List[str] List of all heuristics and algorithms against which to benchmark
test_heuristics: ['rand', 'EDD', 'SPT', 'MTR', 'LTR']
# (O)   [str]     Metric name in the final evaluation table which summarizes the training success best. See
                  # EvaluationHandler.evaluate_test() in utils.evaluations for suitable metrics or add one.
                  # In a wandb hyperparameter sweep this will be usable as objective metric.
success_metric:   makespan_mean